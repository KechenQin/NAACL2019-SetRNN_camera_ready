\section{Conclusion}
In this work, we present an adaptation of RNN sequence models to the problem of multi-label classification for text. RNN only directly defines probabilities for sequences, but not for sets. Different from previous approaches, which either transform a set to a sequence in some pre-specified order, or relate the sequence probability to the set probability in some ad hoc way, our formulation is derived from a principled notion of set probability. We define the set probability as the sum of all corresponding sequence permutation probabilities. We derive a new training objective that maximizes the set probability and a new prediction objective that finds the most probable set. These new objectives are theoretically more appealing than existing ones, because they give the RNN model more freedom to automatically discover and utilize the best label orders.%\kechen{or we can move this to the intro: There’s a good analogy to be made in §2 to most probable parse vs most probable sentence in parsing. One sentence can be represented by multiple parses, and one set can be represented by many orderings. Extracting what we want would require marginalizing over structures (as you explain later). There’s also the idea of Viterbi EM versus regular EM; the former only uses the best structure.}
 % We develop efficient procedures to tackle the computations difficulties involved in training and prediction. Experiments on benchmark data sets demonstrate that our method outperforms state-of-the-art methods for this task, especially on rare labels.

\label{sec:conclusion}